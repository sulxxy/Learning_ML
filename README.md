# Learning Machine Learning
Theory and implementations (via sci-kit, Pytorch, Tensorflow, jupyter lab, etc.) of machine learning and deep learning.
# Content
1. [Fundamentals](https://github.com/sulxxy/Learning_ML#fundamentals)
1. [Advances](https://github.com/sulxxy/Learning_ML#advances)
1. [Deep Learning Frameworks](https://github.com/sulxxy/Learning_ML#deep-learning-frameworks)
1. [Deep Learning In Practice](https://github.com/sulxxy/Learning_ML#deep-learning-in-practice)
1. [Applications](https://github.com/sulxxy/Learning_ML#applications)
1. [Research Topics](https://github.com/sulxxy/Learning_ML#research-topics)

# Fundamentals
- [ ] [Bayes Decision Theory](https://github.com/sulxxy/Learning_ML/tree/master/Basics/BayesDecisionTheory)
- [ ] [Maximum Likelihood Estimation](https://github.com/sulxxy/Learning_ML/tree/master/Basics/MaximumLikelihoodEstimation)
- [ ] [Bayes Paramter Estimation](https://github.com/sulxxy/Learning_ML/tree/master/Basics/BayesParameterEstimation)
- [ ] [Model Selection](https://github.com/sulxxy/Learning_ML/tree/master/Basics/ModelSelection)
- [ ] [Linear Discriminent Analysis](https://github.com/sulxxy/Learning_ML/tree/master/Basics/LDA)
- [x] [Support Vector Machine](https://github.com/sulxxy/Learning_ML/tree/master/Basics/SVM)
- [x] [Perceptron and MLP](https://github.com/sulxxy/Learning_ML/tree/master/Basics/MLP/)
- [x] [Back Propagation](https://github.com/sulxxy/Learning_ML/tree/master/Basics/BackProp/)
- [ ] [K-means](https://github.com/sulxxy/Learning_ML/tree/master/Basics/K-means)
- [ ] [EM and Gaussian Mixture Model](https://github.com/sulxxy/Learning_ML/tree/master/Basics/EM_and_GMM)
- [ ] [Gaussian Process](https://github.com/sulxxy/Learning_ML/tree/master/Basics/GaussianProcess)
- [ ] [Principle Component Analysis](https://github.com/sulxxy/Learning_ML/tree/master/Basics/PCA)
- [ ] [Independent Component Analysis](https://github.com/sulxxy/Learning_ML/tree/master/Basics/ICA)
- [ ] [Regularization]

# Advances
- [ ] [Sampling](https://github.com/sulxxy/Learning_ML/tree/master/Basics/Sampling)
- [ ] [T-distributed Stochastic Neighbor Embedding (T-SNE)](https://github.com/sulxxy/Learning_ML/tree/master/Advances/TSNE)
- [ ] [Local Linear Embedding (LLE)]()
- [ ] [Canonical Correlation Analysis (CCA)]()
- [ ] [Hidden Markov Models (HMM)]()
- [ ] [One-class SVMs]()
- [ ] [Relevant Dimensionality Estimates (RDE)]()
- [ ] [Explaining ML]()
- [ ] [Reinforcement Learning](https://github.com/sulxxy/Learning_ML/tree/master/Advances/ReinforcementLearning)

# Deep Learning Frameworks
- [x] [Pytorch](https://github.com/sulxxy/Learning_ML/tree/master/Frameworks/Pytorch/pytorch_tutorial.ipynb)
- [x] [Tensorflow](https://github.com/sulxxy/Learning_ML/tree/master/Frameworks/Tensorflow/CNN/CNN.ipynb)

# Deep Learning In Practice
- [ ] [Neural Networks]()
- [x] [Convolutional Neural Networks]()
- [ ] [Recurrent Neural Networks and Long Short-Term Memory Networks]()
- [ ] [Autoencoders]()
- [ ] [Restricted Boltzmann Machines]()
- [ ] [Residual Networks]()
- [ ] [Generative Adversarial Networks]()


# Applications
- [ ] [Natural Language Processing]()
- [ ] [Machine Translation]()
- [ ] [Computer Vision]()
- [ ] [Object Detection]()
- [ ] [Object Segmentation]()
- [ ] [Recommendation Systems]()

# Research Topics
I create another [repo](https://github.com/sulxxy/Machine_Learning_Paper_Reviews) for research paper explanations in machine learning area. For convenience, here is the paper list.
1. A. Cutkosky and K. Boahen, “Online Learning Without Prior Information,”Proceedings of MachineLearning Research, vol. 65, pp. 1–35, 2017.
2. S. Shalev-Shwartzet al., “Online learning and online convex optimization,”Foundations and TrendsR©in Machine Learning, vol. 4, no. 2, pp. 107–194, 2012.
3. A. Cutkosky and F. Orabona, “Black-Box Reductions for Parameter-free Online Learning in BanachSpaces,” feb 2018.
4. F. Orabona and D. Pál, “Coin Betting and Parameter-Free Online Learning,” feb 2016.
5. F. Orabona and T. Tommasi, “Training Deep Networks without Learning Rates Through CoinBetting,” may 2017.
6. E. Hazan, K. Y. Levy, and S. Shalev-Shwartz, “On Graduated Optimization for Stochastic Non-ConvexProblems,” mar 2015.
7. P. L. Bartlett, E. Hazan, and A. Rakhlin, “Adaptive Online Gradient Descent,”Advances in neuralinformation processing systems, vol. 13, pp. 1–8, 2007.
